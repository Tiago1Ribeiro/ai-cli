# DEV_LOG

## 2025-01-17 - MemÃ³ria de Conversas

### O que foi feito

Implementada memÃ³ria de conversa usando `llm -c` nativo! ğŸ§ 

**Uso**:
```bash
ai "olÃ¡, chamo-me JoÃ£o"           # Nova conversa
ai -c "qual Ã© o meu nome?"        # Continua - modelo sabe!
ai -c "explica melhor"            # Continua contexto
ai "nova pergunta"                # Nova conversa (sem -c)
```

**ImplementaÃ§Ã£o**:
- Flag `-c` / `--continue` no comando principal
- Passa `-c` para o `llm` CLI (usa BD interna de conversas)
- Quando continua, nÃ£o reenvia system prompt (jÃ¡ foi enviado)
- Transparente para o utilizador

**Vantagem**: Usa o sistema de conversas nativo do `llm` em vez de implementar ficheiros prÃ³prios.

---

## 2025-01-17 - UI Premium: PainÃ©is e Spinner

### O que mudou
SubstituÃ­do o streaming de texto "cru" por uma experiÃªncia mais polida:
1. **Spinner de espera**: `â ‹ A pensar...` enquanto o modelo gera a resposta completa.
2. **Painel de Resposta**: Quando a resposta estÃ¡ pronta, Ã© apresentada dentro de uma **caixa com bordas arredondadas e azuis**.
3. **Largura Limpa**: Respostas limitadas a 100 caracteres de largura (ou largura do terminal) para melhor leitura.

### PorquÃª?
- O streaming misturado com markdown por vezes quebrava a formataÃ§Ã£o visual.
- PainÃ©is dÃ£o um aspeto muito mais "profissional" e organizado ao CLI.
- Evita "flicker" de renderizaÃ§Ã£o.

---

## 2025-01-17 - Fix: Comandos Seguros NÃ£o Executavam

### Problema

O modelo **inventava** resultados em vez de executar comandos!

```bash
ai "o que tem este repo?"
# â†’ "vou usar ls"
# â†’ "NÃ£o tenho output, vou SUPOR que tem..."
# â†’ INVENTA ficheiros fake! ğŸ˜±
```

### Causa

A funÃ§Ã£o `execute_safe_commands` existia mas estava **desatualizada**:
- Esperava `dict` mas new `safe_commands.py` retorna `CommandResult` dataclass
- NÃ£o tratava `result.output` corretamente
- Metadata nÃ£o era acedida

### SoluÃ§Ã£o

Corrigida integraÃ§Ã£o em `llm_client.py`:
- âœ… `_format_command_result` agora trata `CommandResult` (dataclass)
- âœ… Acede `result.output`, `result.metadata`, `result.error`
- âœ… Cria `CommandResult` para erros


Agora comandos **EXECUTAM A SÃ‰RIO**! ğŸ¯

---

## 2025-01-17 - Melhorias de SeguranÃ§a nas Tools

### O que foi feito

Corrigidos problemas **crÃ­ticos de seguranÃ§a** e performance nas tools!

**safe_commands.py - Security Hardening**:
- ğŸ”’ **Path traversal BLOQUEADO** - `../../../etc/passwd` jÃ¡ nÃ£o funciona!
- ğŸ”’ **Paths bloqueados** - `.env`, `.ssh`, `/etc/passwd`, `secrets.json`
- âœ… **Binary file detection** - NÃ£o lÃª mais ficheiros binÃ¡rios (lixo)
- âœ… **Type hints corretos** - `Any` em vez de `any`
- âœ… **Security levels** - STRICT/NORMAL/RELAXED
- âœ… **CommandResult dataclass** - Em vez de `dict[str, any]`
- âœ… **Mimetype detection** -Verifica extensÃ£o + mimetype + bytes

**tree.py - Performance & Features**:
- âš¡ **Limites de items** - Max 100 ficheiros por dir, 1000 total
- âš¡ **RecursÃ£o segura** - MAX_DEPTH_SAFETY=20 (evita stack overflow)
- âœ… **Suporte .gitignore** - LÃª e respeita .gitignore do projeto
- âœ… **JSON output** - `ai tree --json > tree.json`
- âœ… **EstatÃ­sticas** - Mostra dirs/files/size no final
- âœ… **Emoji fallback** - DetecÃ§Ã£o de suporte a Unicode
- ğŸ”’ **`-a` seguro** - SÃ³ mostra hidden, nÃ£o remove TODOS os ignores

**Antes vs Depois**:
```bash
# ANTES - VULNERÃVEL!
ai "lÃª o ficheiro /etc/passwd"  # â† Funcionava! ğŸ˜±
ai tree  # Em node_modules â†’ TRAVA

# AGORA - SEGURO!
ai "lÃª o ficheiro /etc/passwd"  # â† Bloqueado! âœ…
ai tree  # â†’ Max 100 items/dir, estatÃ­sticas
```

**Security levels**:
- `STRICT` (default): SÃ³ paths dentro do CWD
- `NORMAL`: Paths relativos ok, /etc bloqueado
- `RELAXED`: Qualquer path (sÃ³ para debug!)

Agora as tools sÃ£o **production-ready** e seguras! ğŸ”

---

## 2025-01-17 - Sistema de Troca DinÃ¢mica de Modelo

### O que foi feito

Implementado sistema completo de gestÃ£o de modelos com persistÃªncia!

**Features principais**:
1. âœ… **PersistÃªncia de configuraÃ§Ã£o** - Guardada em `~/.config/ai-cli/config.json` (XDG compliant)
2. âœ… **Modelo default dinÃ¢mico** - NÃ£o Ã© mais hardcoded!
3. âœ… **Modelos custom** - Adiciona quantos quiseres
4. âœ… **Menu interativo** - SeleÃ§Ã£o visual com Rich
5. âœ… **HistÃ³rico de uso** - Top 5 modelos recentes aparecem primeiro

**Novos comandos**:
```bash
ai model             # Menu interativo
ai model list        # Lista todos (built-in + custom)
ai model set fast    # Define default
ai model add meu gpt-4 "DescriÃ§Ã£o"  # Adiciona custom
ai model remove meu  # Remove custom
ai model current     # Mostra atual
ai model reset       #Reset config
ai model info        # Info do ficheiro config
```

**Estrutura do config.json**:
```json
{
  "default_model": "fast",
  "custom_models": {
    "claude": {
      "alias": "claude",
      "model_id": "claude-3-opus",
      "description": "Claude para escrita",
      "is_custom": true
    }
  },
  "recent_models": ["fast", "maverick"],
  "system_prompt": "...",
  "stream_by_default": true
}
```

**Melhorias automÃ¡ticas**:
- âœ… Se modelo custom removido era default â†’ reverte para "maverick"
- âœ… Modelos built-in nÃ£o podem ser removidos/substituÃ­dos
- âœ… Config corrupta? Usa defaults automaticamente
- âœ… Cross-platform (Windows/Linux/Mac)

**Uso**:
```bash
# Antes (sempre hardcoded):
ai -m fast "pergunta"

# Agora (persiste escolha):
ai model set fast
ai "pergunta"  # â† Usa 'fast' automaticamente!
```

Agora o CLI Ã© **muito** mais flexÃ­vel! ğŸ¯

---

## 2025-01-17 - Melhorias na RenderizaÃ§Ã£o Markdown

### O que foi feito

Melhorada a renderizaÃ§Ã£o de markdown para melhor legibilidade e aparÃªncia:

**Problemas corrigidos**:
1. âœ… Quebras de linha inconsistentes
2. âœ… CÃ³digo inline nÃ£o formatado (agora: preto em fundo branco)
3. âœ… Links nÃ£o destacados (agora: cyan sublinhado)
4. âœ… Blocos de cÃ³digo muito espaÃ§ados (mais compactos agora)
5. âœ… Linhas vazias excessivas

**Melhorias implementadas**:
- Tema customizado com boa legibilidade
- `soft_wrap=True` para quebras de linha suaves
- **CÃ³digo inline**: preto em fundo branco (muito visÃ­vel!)
- **Links**: cyan sublinhado
- Blocos de cÃ³digo sem nÃºmeros de linha (mais compacto)
- Processamento de texto para limpar linhas vazias excesso
- Ãcones Unicode nos erros (âœ—, âœ“, âš , â„¹)
- `highlight=True` para destacar automaticamente

**Resultado**: Output muito mais limpo, legÃ­vel e bem formatado! ğŸ¨

---

## 2025-01-17 - CriaÃ§Ã£o do CLI-AI

### O que foi feito

Criado sistema CLI em Python para interagir com modelos LLM via terminal.

**Estrutura criada:**
```
cli-ai/
â”œâ”€â”€ src/ai_cli/
â”‚   â”œâ”€â”€ __init__.py      # VersÃ£o e metadata
â”‚   â”œâ”€â”€ main.py          # Entry point com Click
â”‚   â”œâ”€â”€ config.py        # Modelos e configuraÃ§Ã£o
â”‚   â”œâ”€â”€ llm_client.py    # Interface com llm (datasette)
â”‚   â”œâ”€â”€ render.py        # RenderizaÃ§Ã£o markdown (Rich)
â”‚   â””â”€â”€ tools/
â”‚       â”œâ”€â”€ tree.py      # Estrutura de pastas
â”‚       â”œâ”€â”€ find.py      # Pesquisa com ripgrep
â”‚       â””â”€â”€ fzf.py       # Fuzzy finder
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_render.py
â”‚   â”œâ”€â”€ test_config.py
â”‚   â””â”€â”€ test_cli.py
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â””â”€â”€ .gitignore
```

**Comandos disponÃ­veis:**
- `ai pergunta aqui` - Pergunta ao modelo (sem aspas)
- `ai -m fast pergunta` - Modelo especÃ­fico
- `ai file ficheiro.py` - Analisar ficheiro
- `ai explain ficheiro.py` - Explicar cÃ³digo
- `ai tree` - Estrutura de pastas com Ã­cones
- `ai find pattern` - Pesquisa com ripgrep
- `ai fzf` - Fuzzy finder interativo
- `ai models` - Listar modelos

**Stack:**
- Python 3.9+
- click (CLI framework)
- rich (markdown rendering)
- llm (datasette - backend LLM)

---

## 2025-01-17 - CorreÃ§Ãµes de Testes

### O que foi feito

Corrigidos 2 testes que falhavam na test suite:

1. **`test_get_model_invalid`** em `tests/test_config.py`:
   - Problema: Mensagem de erro tinha "nao encontrado" (sem acento)
   - Teste esperava: "nÃ£o encontrado" (com acento)
   - **SoluÃ§Ã£o**: Corrigido acento em `src/ai_cli/config.py:62`

2. **`test_find_help`** em `tests/test_cli.py`:
   - Problema: Teste verificava "pattern" mas nÃ£o estava no output do `--help`
   - **SoluÃ§Ã£o**: Mudado para verificar "ripgrep" ou "pesquisa" que estÃ£o no help

**Resultado**: 17/17 testes a passar âœ…

---

## 2025-01-17 - CorreÃ§Ã£o de Encoding Console Windows

### O que foi feito

Corrigido problema de acentos corrompidos no output (`mï¿½quina` em vez de `mÃ¡quina`):

**Problema**: 
- Windows console usa codepage diferente (cp1252, cp850) e nÃ£o UTF-8
- ForÃ§ar UTF-8 causava caracteres corrompidos nos acentos portugueses

**SoluÃ§Ã£o**:
1. Adicionado `import locale` em `llm_client.py`
2. Criada variÃ¡vel `SYSTEM_ENCODING = locale.getpreferredencoding(False) or "utf-8"`
3. SubstituÃ­do `encoding="utf-8"` por `encoding=SYSTEM_ENCODING` nos subprocess calls

Agora o CLI detecta automaticamente o encoding correto do sistema Windows e exibe acentos corretamente! ğŸ‡µğŸ‡¹

---

## 2025-01-17 - Contexto do Sistema no Prompt

### O que foi feito

Adicionado contexto automÃ¡tico do sistema operativo ao prompt do LLM para melhor awareness.

**InformaÃ§Ãµes incluÃ­das no contexto**:
- âœ… Sistema Operativo (Windows/Linux/macOS)
- âœ… Username do sistema
- âœ… DiretÃ³rio atual
- âœ… Git branch (se estiver num repositÃ³rio)
- âœ… Shell utilizado (cmd.exe, powershell, bash, etc.)
- âœ… Timestamp da execuÃ§Ã£o
- âœ… IndicaÃ§Ã£o que estÃ¡ via CLI

**Exemplo de contexto adicionado**:
```
CONTEXTO DO SISTEMA:
- SO: Windows
- User: mediaweb.global
- Diretorio: C:\projectos\cli-ai
- Git branch: main
- Shell: cmd.exe
- Timestamp: 2025-01-17 14:58
- CLI: ai-cli
```

**DetecÃ§Ã£o inteligente**:
- Git branch Ã© detectado automaticamente (com timeout de 1s para nÃ£o atrasar)
- Username funciona em Windows (USERNAME) e Unix (USER)
- Shell detecta cmd.exe, powershell, bash, etc.

Agora o modelo tem awareness completo do ambiente! ğŸ¯

---

## 2025-01-17 - RenderizaÃ§Ã£o Markdown no Output

### O que foi feito

Ativada a renderizaÃ§Ã£o markdown nas respostas do LLM para melhor legibilidade.

**Problema**:
- Respostas eram mostradas em plain text
- **Bold** aparecia como `**bold**`
- Listas e cÃ³digo nÃ£o tinham formataÃ§Ã£o

**SoluÃ§Ã£o**:
1. Modificado `query_llm()` em `llm_client.py` para acumular a resposta completa
2. Usar `render_markdown()` para renderizar a resposta formatada
3. Aplicado tanto em modo streaming como nÃ£o-streaming

**Agora suportado**:
- âœ… **Bold** e *itÃ¡lico*
- âœ… `cÃ³digo inline`
- âœ… Blocos de cÃ³digo com syntax highlighting
- âœ… Listas numeradas e bullet points
- âœ… Headers (#, ##, ###)
- âœ… Links e citaÃ§Ãµes

Output muito mais bonito e legÃ­vel! ğŸ¨

---

## 2025-01-17 - ExecuÃ§Ã£o de Comandos Seguros

### O que foi feito

Implementado sistema de execuÃ§Ã£o automÃ¡tica de comandos seguros (read-only) que o modelo pode usar.

**Arquitetura**:
1. Criado `tools/safe_commands.py` com funÃ§Ãµes seguras
2. Atualizado system prompt para informar o modelo sobre comandos disponÃ­veis
3. Parser que detecta `[CMD: ...]` tags na resposta e executa automaticamente
4. Resultados sÃ£o inseridos inline na resposta

**Comandos seguros disponÃ­veis**:
- âœ… `[CMD: ls]` ou `[CMD: ls caminho]` - Listar directÃ³rio
- âœ… `[CMD: cat ficheiro.txt]` - Ler ficheiro (max 100 linhas, max 1MB)
- âœ… `[CMD: pwd]` - Mostrar diretÃ³rio atual
- âœ… `[CMD: git status]` - Ver status do git
- âœ… `[CMD: git log]` - Ver Ãºltimos 5 commits

**SeguranÃ§a**:
- âœ… Apenas comandos READ-ONLY
- âœ… Timeout de 5 segundos
- âœ… ProteÃ§Ã£o contra ficheiros grandes (>1MB)
- âœ… Limite de 100 linhas por ficheiro
- âœ… Error handling robusto

**Exemplo de uso**:
```
User: ai o que tens na pasta atual?
Model: Vou ver: [CMD: ls]
        
        [Executa automaticamente e mostra resultado]
        
        dir  src
        dir  tests
        file README.md
        file pyproject.toml
```

O modelo agora pode "ver" o sistema de ficheiros! ğŸ”
