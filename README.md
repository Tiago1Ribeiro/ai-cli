# AI CLI

Assistente de IA vers√°til para terminal, alimentado por LLMs, com execu√ß√£o de comandos, renderiza√ß√£o rica de markdown e mem√≥ria de contexto.

![alt text](assets\leiria_existe.png)

## Caracter√≠sticas

- **Mem√≥ria de Conversa√ß√£o**: Reten√ß√£o adequada de contexto com flag `-c`
- **C√≥pia Autom√°tica**: Respostas automaticamente copiadas para clipboard
- **Ferramentas Integradas**: Executa comandos do sistema (ls, cat, tree, find, git)
- **Renderiza√ß√£o Rica**: TUI minimalista e bonita com syntax highlighting
- **Streaming em Direto**: Resposta em tempo real com anima√ß√£o
- **Modelos Din√¢micos**: Adiciona, alterna e gere facilmente diferentes backends LLM

## Come√ßar (Quick Start)

Segue estes passos para instalar e configurar o AI CLI.

### 1. Instalar AI CLI (M√©todo Recomendado: pipx)

O **pipx** instala aplica√ß√µes Python isoladamente, mas torna o comando `ai` dispon√≠vel globalmente. N√£o precisas de ativar ambientes virtuais para usar o comando.

```bash
# 1. Instalar pipx (uma vez)
python -m pip install --user pipx
python -m pipx ensurepath

# 2. Clonar o reposit√≥rio
git clone https://github.com/Tiago1Ribeiro/ai-cli.git
cd ai-cli

# 3. Instalar ai-cli globalmente (modo edit√°vel para desenvolvimento)
pipx install -e .

# 4. Reinicia o terminal e verifica
ai --version
```

**Vantagens do pipx:**
- ‚úÖ Comando `ai` dispon√≠vel em qualquer pasta, qualquer terminal
- ‚úÖ Sem necessidade de `conda activate` ou ativar venv
- ‚úÖ Isolamento total das depend√™ncias
- ‚úÖ Modo edit√°vel: altera o c√≥digo em `src/` e as mudan√ßas refletem-se imediatamente

### Alternativa: Instala√ß√£o Manual (Para Desenvolvimento Avan√ßado)

Se preferires controlo total sobre o ambiente:

**Op√ß√£o A: Usando `venv`**
```bash
# Windows
python -m venv .venv
.venv\Scripts\activate

# Linux/Mac
python3 -m venv .venv
source .venv/bin/activate

# Instalar
pip install -e .
```

**Op√ß√£o B: Usando `conda`**
```bash
conda create -n ai-cli python=3.10
conda activate ai-cli
pip install -e .
```

**Nota**: Com esta op√ß√£o, ter√°s de ativar o ambiente sempre que quiseres usar o comando `ai`.

### 2. Instalar Depend√™ncia Core

O `ai-cli` depende da ferramenta `llm` para gerir modelos:

```bash
# Se instalaste via pipx (recomendado)
pipx inject ai-cli llm

# Se instalaste via pip/conda
pip install llm
```

### 3. Configurar Modelos (Recomendado: Free Tier)

O AI CLI usa a ferramenta `llm` para gerir modelos. Recomendamos fornecedores com **Free Tier** generoso e r√°pido.

#### Op√ß√£o A: Groq (Recomendado - Gr√°tis e Ultra-R√°pido)
Ideal para respostas instant√¢neas. Modelo `llama-3.3-70b-versatile` gr√°tis com muita quota.

1. **Obt√©m uma API Key gr√°tis** em [console.groq.com](https://console.groq.com)
2. **Instala o plugin Groq:**
```bash
pipx inject ai-cli llm-groq
```

3. **Configura a API Key:**
```bash
# Windows (PowerShell)
$env:PIPX_HOME = "$env:USERPROFILE\pipx"
& "$env:PIPX_HOME\venvs\ai-cli\Scripts\python.exe" -m llm keys set groq
# Cola a tua chave quando pedido (gsk_...)

# Linux/Mac
python -m llm keys set groq
```

4. **Define o modelo padr√£o:**
```bash
# Windows
& "$env:PIPX_HOME\venvs\ai-cli\Scripts\python.exe" -m llm models default llama-3.3-70b-versatile

# Linux/Mac
python -m llm models default llama-3.3-70b-versatile
```

5. **Testa:**
```bash
ai ol√° mundo
```

#### Op√ß√£o B: Ollama (Local e Privado)
Se preferes rodar localmente sem internet:

1. **Instala Ollama:** [ollama.com/download](https://ollama.com/download)
2. **Faz pull de um modelo:**
```bash
ollama pull llama3.2
```

3. **Instala o plugin:**
```bash
pipx inject ai-cli llm-ollama
```

4. **Define como padr√£o:**
```bash
pipx inject ai-cli llm-cloudflare

# Windows
& "$env:PIPX_HOME\venvs\ai-cli\Scripts\python.exe" -m llm keys set cloudflare

# Linux/Mac
python -m llm keys set cloudflare
# Linux/Mac
python -m llm models default llama3.2
```

#### Op√ß√£o C: Cloudflare Workers AI (Gr√°tis)
```bash
llm install llm-cloudflare
llm keys set cloudflare
# Segue as instru√ß√µes do plugin
```

### 4. Testar

Agora que tens um modelo padr√£o definido no `llm`, o `ai` vai us√°-lo automaticamente:

```bash
ai ol√° mundo
```

### üèÉ Quick Reference - Mudar de Modelo

```bash
# Usar modelo diferente uma vez
ai -m llama-3.1-8b-instant pergunta r√°pida

# Mudar modelo default permanentemente (Windows)
$env:PIPX_HOME = "$env:USERPROFILE\pipx"
& "$env:PIPX_HOME\venvs\ai-cli\Scripts\python.exe" -m llm models default llama-3.3-70b-versatile

# Ver modelos dispon√≠veis
ai --models

# Menu interativo
ai model
```

**üí° Dica:** Se instalaste com pipx e precisas de executar comandos `llm` diretamente, usa:
```bash
# Windows
$env:PIPX_HOME = "$env:USERPROFILE\pipx"
& "$env:PIPX_HOME\venvs\ai-cli\Scripts\python.exe" -m llm [comando]

# Exemplo: Listar modelos
& "$env:PIPX_HOME\venvs\ai-cli\Scripts\python.exe" -m llm models list
```

## Configura√ß√£o

### Configura√ß√£o Inicial

Na primeira execu√ß√£o, AI CLI cria um ficheiro de configura√ß√£o:

- **Windows**: `%APPDATA%\ai-cli\config.json`
- **Linux/macOS**: `~/.config/ai-cli/config.json`

### Configurar Modelos

AI CLI vem com aliases de modelos pr√©-configurados. Adiciona os teus modelos:

```bash
# Gest√£o interativa de modelos
ai model

# Adicionar um modelo personalizado
ai model add mymodel gpt-4o "O meu modelo GPT-4o"

# Definir modelo padr√£o
ai model set mymodel

# Listar todos os modelos configurados
ai model list
```

### Ficheiro de Configura√ß√£o de Modelos

Edita `config.json` para personalizar modelos ou aliases de conveni√™ncia:

```json
{
  "default_model": null, 
  "models": {
    "fast": {
      "model_id": "llama-3.3-70b-versatile",
      "description": "Llama 3.3 70B via Groq - R√°pido e inteligente"
    },
    "quick": {
      "model_id": "llama-3.1-8b-instant",
      "description": "Llama 3.1 8B via Groq - Ultra-r√°pido"
    },
    "local": {
      "model_id": "llama3.2",
      "description": "Llama 3.2 local via Ollama"
    }
  }
}
```

**Nota**: Se `default_model` for null, o AI CLI usa o modelo padr√£o definido globalmente no `llm` (v√™ comando na sec√ß√£o Troubleshooting).

**Modelos Groq Populares (gratuitos):**
- `llama-3.3-70b-versatile` - Mais inteligente
- `llama-3.1-8b-instant` - Mais r√°pido
- `mixtral-8x7b-32768` - Contexto longo
- `gemma2-9b-it` - Eficiente

## Uso

### Queries B√°sicas (Sem aspas!)

```bash
# Pergunta simples - SEM ASPAS
ai qual √© a capital de Fran√ßa

# Queries multi-palavra - SEM ASPAS
ai explica computa√ß√£o qu√¢ntica em termos simples
```

### Mudar de Modelo

Tens 3 formas de mudar de modelo:

#### 1. Temporariamente (apenas para esta query)
```bash
# Usar flag -m com qualquer modelo instalado no llm
ai -m llama-3.1-8b-instant pergunta r√°pida

# Usar alias configurado no ai-cli
ai -m fast explica isto em detalhe
ai -m quick resposta r√°pida
```

#### 2. Mudar o modelo default do sistema (llm)
Este √© usado por todas as apps que usam `llm`, incluindo o `ai-cli`:

```bash
# Windows PowerShell
$env:PIPX_HOME = "$env:USERPROFILE\pipx"
& "$env:PIPX_HOME\venvs\ai-cli\Scripts\python.exe" -m llm models default llama-3.3-70b-versatile

# Linux/Mac
python -m llm models default llama-3.3-70b-versatile
```

#### 3. Usar o menu interativo (mais f√°cil)
```bash
ai model               # Menu interativo - escolhe modelo
ai model set fast      # Define 'fast' como default (alias do ai-cli)
ai model current       # Ver modelo atual
```

**Recomenda√ß√£o:** Usa a Op√ß√£o 2 (default do llm) para um modelo "global", e a flag `-m` quando precisares de outro temporariamente.

### Comandos do Sistema (com --)

```bash
ai --help              # Mostrar ajuda
ai --version           # Mostrar vers√£o
ai --config            # Mostrar configura√ß√£o
ai --models            # Listar modelos dispon√≠veis
ai --check             # Verificar estado do sistema
```

### Aliases (Atalhos)

Poupa tempo com aliases integrados:

```bash
ai f README.md resume   # Igual a: ai file
ai e main.py            # Igual a: ai explain
ai t                    # Igual a: ai tree
ai s TODO               # Igual a: ai find (search)
```

### Continuar Conversa√ß√µes

```bash
# Primeira mensagem
ai explica redes neuronais

# Continuar a conversa (mant√©m contexto)
ai -c d√°-me um exemplo de c√≥digo

# Continuar
ai -c explica isso com mais detalhe
```

### An√°lise de Ficheiros

```bash
# Analisar um ficheiro
ai file README.md resume este ficheiro

# Explicar c√≥digo
ai explain src/main.py

# Input via pipe
cat error.log | ai o que causou este erro
echo def hello pass | ai melhora este c√≥digo
```

### Ferramentas Integradas

A IA pode executar comandos seguros quando necess√°rio:

```bash
# A IA pode listar ficheiros
ai mostra-me ficheiros python neste diret√≥rio

# A IA pode ler ficheiros
ai o que cont√©m o config.json

# A IA pode verificar estado do git
ai que ficheiros mudaram

# A IA pode procurar padr√µes
ai encontra todos os coment√°rios TODO
```

### Ferramentas Diretas (Avan√ßado)

```bash
# Estrutura do projeto
ai tree

# Pesquisa com ripgrep
ai find def main

# Selecionador interativo de ficheiros
ai fzf
```

### Gest√£o de Modelos Avan√ßada

```bash
# Ver todos os aliases configurados no ai-cli
ai --models

# Menu interativo completo (adicionar/remover/configurar)
ai model

# Adicionar novo alias
ai model add myfast llama-3.1-8b-instant "Meu modelo r√°pido"

# Remover alias
ai model remove myfast

# Listar modelos reais instalados no llm
# Windows:
$env:PIPX_HOME = "$env:USERPROFILE\pipx"
& "$env:PIPX_HOME\venvs\ai-cli\Scripts\python.exe" -m llm models list

# Linux/Mac:
python -m llm models list
```

**Nota sobre Aliases vs Modelos:**
- **Aliases** (`fast`, `quick`, etc.) s√£o atalhos configurados no `ai-cli` (ficheiro `config.json`)
- **Modelos** (`llama-3.3-70b-versatile`, etc.) s√£o os IDs reais instalados no `llm`
- Podes usar ambos com a flag `-m`

### Comandos do Sistema

```bash
# Verificar estado do sistema
ai check            # Verifica instala√ß√£o do llm, modelos, config

# Ver configura√ß√£o
ai config           # Mostra config.json atual

# Desativar streaming (mostrar output completo de uma vez)
ai --no-stream explica isto em detalhe
```

## Refer√™ncia de Comandos

### Comando Principal

```
ai [OP√á√ïES] [QUERY]
```

**Op√ß√µes:**
- `-m, --model MODEL` - Especificar modelo a usar
- `-c, --continue` - Continuar conversa anterior
- `-v, --verbose` - Mostrar output detalhado
- `--no-stream` - Desativar streaming (mostrar output completo)
- `-V, --version` - Mostrar vers√£o
- `-h, --help` - Mostrar mensagem de ajuda

### Comandos

- `ai check` - Verificar estado do sistema (instala√ß√£o llm, modelos, config)
- `ai config` - Mostrar/editar configura√ß√£o
- `ai file <caminho> [query]` - Analisar ficheiro(s)
- `ai explain <caminho>` - Explicar ficheiro de c√≥digo
- `ai find <padr√£o>` - Pesquisar padr√µes com ripgrep
- `ai fzf` - Selecionador fuzzy interativo de ficheiros
- `ai model` - Gest√£o de modelos (interativo)
- `ai models` - Listar modelos dispon√≠veis
- `ai tree` - Mostrar estrutura de diret√≥rios

### Aliases

- `ai f` ‚Üí `ai file`
- `ai e` ‚Üí `ai explain`
- `ai t` ‚Üí `ai tree`
- `ai s` ‚Üí `ai find`

### Subcomandos de Modelo

- `ai model list` - Listar todos os modelos configurados
- `ai model current` - Mostrar modelo padr√£o atual
- `ai model set <alias>` - Definir modelo padr√£o
- `ai model add <alias> <id> <desc>` - Adicionar novo modelo
- `ai model remove <alias>` - Remover modelo

## Caracter√≠sticas em Detalhe

### C√≥pia Autom√°tica para Clipboard

Cada resposta √© automaticamente copiada para o teu clipboard para uso r√°pido. Ver√°s uma confirma√ß√£o:

```
Œª ai-cli ‚Ä¢ 2.3s ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

  [AI response here]

‚à¥ copiado para clipboard ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
```

### Interface Terminal Rica

- **Layout Limpo**: Design minimalista com largura de 80 caracteres
- **Syntax Highlighting**: Blocos de c√≥digo com dete√ß√£o de linguagem
- **Suporte Markdown**: Cabe√ßalhos, listas, tabelas, cita√ß√µes
- **Dete√ß√£o de Caminhos**: Destaca automaticamente caminhos de ficheiros
- **Temas de Cor**: Est√©tica cyan/azul otimizada para legibilidade

### Mem√≥ria de Contexto

Usa `-c` para manter contexto da conversa:

```bash
ai estou a aprender Python          # Primeira mensagem
ai -c recomenda alguns recursos      # IA lembra que est√°s a aprender Python
ai -c e sobre web frameworks?        # IA conhece o contexto
```

### Integra√ß√£o com Sistema

A IA pode executar comandos seguros e s√≥-leitura quando √∫til:

- `ls` / `dir` - Listar ficheiros
- `cat` / `type` - Ler ficheiros
- `pwd` - Diret√≥rio atual
- `git status` - Informa√ß√£o do Git
- `tree` - Estrutura de diret√≥rios
- `find` / `grep` - Pesquisa

**Nota**: Comandos s√≥ s√£o executados quando explicitamente necess√°rios pelo racioc√≠nio da IA. Tu controlas quando as ferramentas executam.

## Resolu√ß√£o de Problemas

### "ai: command not found" (ap√≥s instalar com pipx)

Depois de executar `pipx ensurepath`, precisas de:
1. **Fechar e reabrir o terminal** (ou reiniciar o sistema)
2. Verificar se o PATH foi atualizado:
   ```bash
   # Windows PowerShell
   $env:PATH
   
   # Deve conter algo como:
   # C:\Users\TeuUser\.local\bin
   # C:\Users\TeuUser\AppData\Roaming\Python\Python3XX\Scripts
   ```

Se ainda n√£o funcionar:
```bash
# Usar caminho completo temporariamente (Windows)
C:\Users\TeuUser\.local\bin\ai.exe --help

# Ou executar via Python
python -m pipx run ai --help
```

### "llm: command not found"

### "llm: command not found"

Instala a ferramenta llm CLI:

```bash
# Se usaste pipx (recomendado)
pipx inject ai-cli llm

# Se usaste pip
pip install llm
```

### "No API key configured" ou "Unknown model"

**Setup R√°pido - Groq (Recomendado):**

```bash
# 1. Instalar plugin
pipx inject ai-cli llm-groq

# 2. Configurar chave (obter em console.groq.com)
# Windows PowerShell:
$env:PIPX_HOME = "$env:USERPROFILE\pipx"
& "$env:PIPX_HOME\venvs\ai-cli\Scripts\python.exe" -m llm keys set groq

# 3. Definir modelo default
& "$env:PIPX_HOME\venvs\ai-cli\Scripts\python.exe" -m llm models default llama-3.3-70b-versatile

# 4. Testar
ai ol√°
```

**Outras op√ß√µes:**

```bash
# Para OpenAI (pago)
pipx inject ai-cli openai
# Windows: & "$env:PIPX_HOME\venvs\ai-cli\Scripts\python.exe" -m llm keys set openai
# Linux/Mac: python -m llm keys set openai

# Para Ollama (local/gr√°tis)
ollama pull llama3.2
pipx inject ai-cli llm-ollama
# Windows: & "$env:PIPX_HOME\venvs\ai-cli\Scripts\python.exe" -m llm models default llama3.2
```

### Listar modelos dispon√≠veis

```bash
# Modelos configurados no ai-cli (aliases)
ai --models

# Modelos reais instalados no llm
# Windows:
$env:PIPX_HOME = "$env:USERPROFILE\pipx"
& "$env:PIPX_HOME\venvs\ai-cli\Scripts\python.exe" -m llm models list

# Linux/Mac:
python -m llm models list
```

### Perguntas Frequentes - Modelos

**Q: Qual a diferen√ßa entre `ai -m fast` e `ai -m llama-3.3-70b-versatile`?**
A: `fast` √© um alias (atalho) configurado no `ai-cli` que aponta para `llama-3.3-70b-versatile`. Podes usar qualquer um.

**Q: Como sei que modelo est√° a ser usado?**
A: Usa `ai model current` ou verifica o cabe√ßalho da resposta (mostra dura√ß√£o e modelo).

**Q: Posso ter v√°rios modelos instalados?**
A: Sim! Instala v√°rios plugins (`llm-groq`, `llm-ollama`, etc.) e muda entre eles com `-m` ou alterando o default.

**Q: Como adiciono um modelo que n√£o est√° nos aliases?**
A: Dois m√©todos:
```bash
# M√©todo 1: Usar diretamente o ID do llm
ai -m mixtral-8x7b-32768 tua pergunta

# M√©todo 2: Criar alias
ai model add mix mixtral-8x7b-32768 "Mixtral r√°pido"
ai -m mix tua pergunta
```

**Q: Onde vejo todos os modelos Groq dispon√≠veis?**
A: Depois de instalar `llm-groq`:
```bash
# Windows:
& "$env:PIPX_HOME\venvs\ai-cli\Scripts\python.exe" -m llm models list | Select-String "groq"

# Linux/Mac:
python -m llm models list | grep -i groq
```

### Clipboard n√£o funciona

**Windows**: Deve funcionar automaticamente com `clip.exe`

**macOS**: Deve funcionar com `pbcopy`

**Linux**: Instala `xclip` ou `xsel`:

```bash
# Ubuntu/Debian
sudo apt install xclip

# Fedora
sudo dnf install xclip

# Arch
sudo pacman -S xclip
```

### Caracteres Unicode n√£o aparecem

Certifica-te que est√°s a usar um terminal moderno:
- **Windows**: Windows Terminal (recomendado)
- **macOS**: Terminal.app ou iTerm2
- **Linux**: GNOME Terminal, Konsole ou Alacritty

## Depend√™ncias Opcionais

Melhora a funcionalidade com estas ferramentas opcionais:

- **ripgrep (rg)**: Pesquisa r√°pida para comando `ai find`
  ```bash
  # Windows: winget install BurntSushi.ripgrep.MSVC
  # macOS: brew install ripgrep
  # Linux: sudo apt install ripgrep
  ```

- **fzf**: Selecionador fuzzy interativo para `ai fzf`
  ```bash
  # Windows: winget install fzf
  # macOS: brew install fzf
  # Linux: sudo apt install fzf
  ```

- **tree**: Visualiza√ß√£o de estrutura de diret√≥rios
  ```bash
  # Geralmente pr√©-instalado na maioria dos sistemas
  # Windows: built-in (tree /F)
  ```

## Gest√£o da Instala√ß√£o

### Atualizar o AI CLI
```bash
# Se instalaste com pipx
cd ai-cli
git pull
pipx reinstall ai-cli

# Se instalaste com pip (no venv/conda)
cd ai-cli
git pull
pip install -e . --upgrade
```

### Desinstalar
```bash
# Se instalaste com pipx
pipx uninstall ai-cli

# Se instalaste com pip
pip uninstall ai-cli
```

### Listar aplica√ß√µes instaladas via pipx
```bash
pipx list
```

## Desenvolvimento

### Configurar Ambiente de Desenvolvimento

**M√©todo Recomendado (pipx):**
```bash
# Clonar reposit√≥rio
git clone https://github.com/mediaweb-global/cli-ai.git
cd cli-ai

# Instalar em modo edit√°vel global
pipx install -e .
pipx inject ai-cli llm pytest pytest-cov

# O comando 'ai' est√° agora dispon√≠vel globalmente
# e qualquer altera√ß√£o no c√≥digo em src/ tem efeito imediato
ai --version
```

**M√©todo Tradicional (venv):**
```bash
# Clonar reposit√≥rio
git clone https://github.com/mediaweb-global/cli-ai.git
cd cli-ai

# Criar ambiente virtual
python -m venv venv
source venv/bin/activate  # No Windows: venv\Scripts\activate

# Instalar em modo edit√°vel com depend√™ncias de dev
pip install -e ".[dev]"

# Executar testes
pytest
```

### Estrutura do Projeto

```
cli-ai/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ ai_cli/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ main.py         # Ponto de entrada CLI
‚îÇ       ‚îú‚îÄ‚îÄ config.py       # Gest√£o de configura√ß√£o
‚îÇ       ‚îú‚îÄ‚îÄ llm_client.py   # Intera√ß√£o com LLM
‚îÇ       ‚îú‚îÄ‚îÄ render.py       # Renderiza√ß√£o no terminal
‚îÇ       ‚îî‚îÄ‚îÄ tools/          # Ferramentas integradas
‚îÇ           ‚îú‚îÄ‚îÄ find.py
‚îÇ           ‚îú‚îÄ‚îÄ fzf.py
‚îÇ           ‚îú‚îÄ‚îÄ tree.py
‚îÇ           ‚îî‚îÄ‚îÄ safe_commands.py
‚îú‚îÄ‚îÄ tests/
‚îú‚îÄ‚îÄ pyproject.toml
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ LICENSE
```

## Contribuir

Contribui√ß√µes s√£o bem-vindas! Por favor:

1. Faz fork do reposit√≥rio
2. Cria um branch de feature (`git checkout -b feature/funcionalidade-incrivel`)
3. Faz commit das tuas altera√ß√µes (`git commit -m 'Adiciona funcionalidade incr√≠vel'`)
4. Faz push para o branch (`git push origin feature/funcionalidade-incrivel`)
5. Abre um Pull Request

## Configura√ß√£o

A configura√ß√£o √© guardada em:
- **Windows**: `%APPDATA%\ai-cli\config.json`
- **Linux/Mac**: `~/.config/ai-cli/config.json`

